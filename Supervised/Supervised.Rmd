---
title: 'Análisis Supervisado'
output:
  html_document:
    df_print: paged
---

# Fundamentos de Ingeniería de Datos - Análisis Supervisado

*(2020-2021) José Andrés Pérez, Andrés Martínez*

# Introducción

En este análisis supervisado de datos tenemos como objetivo usar un dataset (ver [1]) correspondiente a un censo de la población activa de EEUU, donde en cada fila del conjunto de datos tenemos disponible información como el género, situación familiar, situación fiscal, educación recibida...

Como clase tenemos una variable categórica binaria que nos indica, para cada ejemplo (que comprende un grupo de personas con características comunes), si tiene unos ingresos brutos anuales superiores a los 50K dólares. En otras palabras, nos indica si ese grupo de personas ha alcanzado el éxito laboral y son capaces de vivir una vida económicamente cómoda.

Por lo tanto, estamos ante un problema de clasificación binaria que pretendemos atacar usando 3 modelos distintos para, finalmente, realizar una comparativa entre ellos vislumbrando las fortalezas y debilidades de cada uno de ellos.

Para este análisis de datos, hemos decidido utilizar las siguientes técnicas:
- Redes Neuronales
- Random Forest
- Support Vector Machine

# Paquetes utilizados

Primero, vamos a instalar los paquetes utilizados para este análisis de datos:

```{r}
install.packages("dplyr")
install.packages("plyr")
install.packages("tidyr")
install.packages("plotrix")
install.packages("VIM")
install.packages("DMwR")
install.packages("Amelia")
install.packages("ggplot2")
install.packages("caTools")
install.packages("rpart")
install.packages("rpart.plot")
install.packages("ISLR")
install.packages("e1071")
install.packages("caret")
install.packages("randomForest")
install.packages("pROC")
install.packages("keras")
install.packages("mice")
install.packages("fastDummies")
#https://datascienceplus.com/imputing-missing-data-with-r-mice-package/


library(plyr)
library(dplyr)
library(tidyr)
library(plotrix)
library(VIM)
library(DMwR)
library(Amelia)
library(ggplot2)
library(caTools)
library(rpart)
library(rpart.plot)
library(ISLR)
library(e1071)
library(caret)
library(randomForest)
library(pROC)
library(keras)
library(mice)
library(fastDummies)
```

# Preprocesamiento de los datos

## Carga y primer vistazo

Antes que nada, vamos a cargar el dataset con los datos a analizar y realizar un vistazo rápido de los mismos. Este primer paso puede ayudarnos a decidir la relevancia de las columnas y su significado.

```{r}
data <- read.csv("../Data/adult.csv")

dim(data)
head(data)
summary(data)
```

Primero vamos a explicar cada atributo para tener clara su intención:
- age: Edad media de los participantes de cada ejemplo
- workclass: variable categórica que indica la clase trabajadora. Puede tener los siguientes valores: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked
- fnlwgt: según [2], se trata del número de personas que se cree que representa cada fila
- education: variable categórica que indica el nivel educativo de cada muestra. Sus valores posibles son: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool
- education.num: asignación numérica al atributo education. Cuanto más alto sea este valor, mayor es el nivel educativo de la muestra
- marital.status: estado civil de la muestra. Los valores posibles son: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse
- occupation: describe el sector laboral de la muestra. En este dataset existen estos posibles valores: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm- clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces
- relationship: indica la situación familiar de la muestra. Los valores posibles son: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried
- race: variable categórica que indica la raza de cada ejemplo. Se han encontrado los siguientes valores: White, Asian-Pac-Islander, Amer-Indian-Eskimo,Black, other
- sex: género binario de la muestra. Contempla únicamente "Male" y "Female"
- capital.gain: expectativa de aumento de ingresos anuales en el siguiente año
- capital.loss: expectativa de pérdida de ingresos anuales en el siguiente año
- hours.per.week: número de horas trabajadas por semana
- native.country: indica el país de origen de la muestra
- income: la clase del dataset. Indica 2 posibles valores: <=50K o >50K

## Visualización de los datos

Primero, realicemos un diagrama de sectores para ver la predominancia de cada valor de la clase:

```{r}
income <- data %>% group_by(data$income) %>% tally()
slices <- income[[2]]
lbls <- income[[1]]
pie3D(slices, labels = lbls, explode = 0.1, theta=1.2, shade=0.4,
main = "Valores de la clase \"income\"")
```

Vemos que alrededor del 75% de los casos pertenecen a la clase <= 50K, por lo que tenemos un dataset algo desbalanceado. En un apartado posterior, aplicaremos el algoritmo SMOTE para aumentar los casos de la clase minoritaria.

Ahora, realicemos unos histogramas sobre varias columnas para observar el rango de valores de cada una de ellas:

```{r}
hist(data$age)
hist(data$fnlwgt)
hist(data$education.num)
```

Sobre estos histogramas podemos sacar varias conclusiones:
- La edad media de las muestras tiene un pico en los 20, 30 y 40 años, y va disminuyendo hasta los 80/90. Quizás deberíamos eliminar las filas con edad mayor a los 70 años porque pueden corresponder a casos muy raros que poco tengan que aportar al estudio
- Cada muestra tiene un censo variable que suele rondar las 100.000 personas, siendo muy infrecuente tener más de 500.000 personas en una muestra
- El nivel educativo tiene un gran pico en el nivel 8 y 9, que corresponden a personas que se han quedado en el curso 12th (no han terminado la secundaria pero casi), y personas que han terminado la secundaria pero no han continuado sus estudios
- Tenemos otro pico en el nivel 12 que corresponde a personas que han llegado a la universidad y han completado una carrera, pero no han continuado los estudios

A continuación, vamos a desglosar el dataset por sector productivo y nivel educativo para ver el número de horas trabajadas:

```{r}
qplot(workclass, hours.per.week, data=data, geom="boxplot", fill=workclass)+
  theme(plot.title=element_text(size=18),axis.text.x=element_text(angle=90,vjust=1))
qplot(education, hours.per.week, data=data, geom="boxplot", fill=education)+
  theme(plot.title=element_text(size=18),axis.text.x=element_text(angle=90,vjust=1))
```
Se ve que los autónomos son los que más horas trabajan a la semana, aunque también hay personas que no cobran y están siendo explotadas, trabajando casi 50 horas a la semana.

En cuanto al nivel educativo, podemos intuir que las personas con un nivel educativo universitario en adelante (máster, doctorado, profesores...) tienden a trabajar más horas a la semana.

Veamos si ese número extra de horas trabajadas tiene correlación con una mayor probabilidad de ganar 50K al año, mostrando esta información mediante diagramas de barras:

```{r}
bar1 <- data %>%
    mutate(ricos = (income == ">50K")) %>%
    group_by(workclass) %>%
    summarize(porcentaje = mean(ricos, na.rm = TRUE) * 100.0)
ggplot(data=bar1, aes(x=workclass, y=porcentaje)) + geom_bar(stat="identity")

bar2 <- data %>%
    mutate(ricos = (income == ">50K")) %>%
    group_by(education) %>%
    summarize(porcentaje = mean(ricos, na.rm = TRUE) * 100.0)
ggplot(data=bar2, aes(x=education, y=porcentaje)) + geom_bar(stat="identity")
```
Y, como sería coherente, el sector autónomo tiene mayor posibilidad de ganar más de 50K al año debido a su mayor número de horas trabajadas semanales.

Si nos fijamos en el diagrama de barras que clasifica por nivel educativo, aquellas personas que tienen estudios superiores a una carrera universitaria (máster y doctorado) y los profesores tienen mayor probabilidad de tener la vida resuelta. Qué suerte que nos encontramos en ese grupo.

Para finalizar, vamos a analizar la edad en cuanto a conseguir el objetivo de ganar 50K al año:

```{r}
bar3 <- data %>% 
  group_by(income, age) %>% 
  tally() %>% 
  complete(age, fill = list(n = 0)) %>% 
  mutate(percentage = n / sum(n) * 100)
ggplot(bar3, aes(age, percentage, fill = income)) + 
  geom_bar(stat = 'identity', position = 'dodge') +
  theme_bw()
```
Donde observamos que antes de los 25 años es bastante improbable conseguir tener esos ingresos anuales (por ejemplo puede deberse a la falta de experiencia), aunque a medida que pasan los años se incrementan sustancialmente las posibilidades, encontrando  un pico entre los 35-45 años, rango de edad donde aún no se es demasiado mayor y se tiene mucha experiencia adquirida.

## Simplificación de atributos

Comencemos renombrando los atributos para que sean más legibles:

```{r}
prepro <- data
names(prepro)[3] <- "people_sample"
names(prepro)[5] <- "education_num"
names(prepro)[6] <- "marital_status"
names(prepro)[11] <- "capital_gain"
names(prepro)[12] <- "capital_loss"
names(prepro)[13] <- "hours_per_week"
names(prepro)[14] <- "native_country"
```

Ahora, podemos eliminar el atributo education_num estableciendo un orden al atributo education, y el atributo people_sample debido a que no proporciona información relevante para realizar la clasificación:

```{r}
education_order <- c("Preschool", "1st-4th", "5th-6th", "7th-8th", "9th", "10th", "11th", "12th", "HS-grad", "Some-college", "Assoc-voc", "Assoc-acdm", "Bachelors", "Masters", "Prof-school", "Doctorate")
prepro$education <- factor(prepro$education, ordered = TRUE, levels = education_order)
prepro$education_num <- NULL
prepro$people_sample <- NULL
```

También podemos convertir el resto de atributos tipo texto a variables categóricas. Recordemos que la clase debe ser un factor para poder aplicar SMOTE:

```{r}
prepro$moreThan50K <- as.factor(prepro$income)
prepro$income <- NULL
prepro$workclass <- as.factor(prepro$workclass)
prepro$marital_status <- as.factor(prepro$marital_status)
prepro$occupation <- as.factor(prepro$occupation)
prepro$relationship <- as.factor(prepro$relationship)
prepro$race <- as.factor(prepro$race)
prepro$sex <- as.factor(prepro$sex)
prepro$native_country <- as.factor(prepro$native_country)
head(prepro)
```
## Tratamiento de dataset desbalanceado

Anteriormente vimos que hay un porcentaje algo bajo de casos de la clase minoritaria (>50K), por lo que vamos a generar ejemplos de esa clase usando el algoritmo SMOTE:

```{r}
prepro <- SMOTE(moreThan50K ~ .,prepro, perc.over=100)
dim(prepro)
```

Vamos a dibujar de nuevo el diagrama de sectores para observar los cambios.

```{r}
income <- prepro %>% group_by(prepro$moreThan50K) %>% tally()
slices <- income[[2]]
lbls <- income[[1]]
pie3D(slices, labels = lbls, explode = 0.1, theta=1.2, shade=0.4,
main = "Valores de la clase \"income\"")
```
Como esperábamos, ahora el porcentaje de casos de cada clase es el mismo, consiguiendo un balance en el dataset.

## Tratamiento de los valores perdidos

Ahora, veamos cuántos valores perdidos tenemos. Al observar el dataset detenidamente, se ha llegado a la conclusión de que los valores perdidos se identifican con una "?" en las variables tipo texto y categóricas. Por lo tanto, vamos a sustituirlos por valores NA:

```{r}
colSums(is.na(prepro))
prepro[prepro == "?"] <- NA
colSums(is.na(prepro))
```
Observemos los valores perdidos realizando un gráfico para hacernos una idea de su distribución:

```{r}
options(repr.plot.width = 15, repr.plot.height = 15)
missmap(prepro, rank.order = FALSE, col = c(0,1), legend = FALSE)
```
Echando un vistazo al diagrama anterior, podemos concluir que los valores perdidos se concrentan principalmente en 3 columnas: workclass, occupation, education y native_country.

Para eliminar los valores perdidos, vamos a aplicar varias técnicas y ver cuál de ellas nos da un mejor resultado. Primero, vamos a eliminar todas las filas que contengan algún valor perdido:

```{r}
prepro_withoutNA <- prepro %>% drop_na()
colSums(is.na(prepro_withoutNA))
missmap(prepro_withoutNA, rank.order = FALSE, col = c(0,1), legend = FALSE)
```
Vemos que los valores perdidos han desaparecido.

A continuación, vamos a usar el algoritmo KNN para, en vez de eliminar las filas directamente, sustituir los valores perdidos por la media del valor de los k vecinos más cercanos:

```{r}
#prepro_knn <- VIM::kNN(prepro, variable = c( 'workclass','education','occupation','native_country'))
#prepro_knn<- subset(prepro_knn, select = -c(14:26))
#save(prepro_knn, file = "prepro_knn.Rdata")
load(file = 'prepro_knn.Rdata')
colSums(is.na(prepro_knn))
head(prepro_knn)

```
Con el paquete mice simplificamos el imputar los MV con algoritmos más complejos y potentes que la media o la mediana, en este caso usaremos el método Predictive Mean Matching prpuesto por Donald B. Rubin el 86. Su objetivo es reducir la bias añadiendo sólo valores reales tomados de otras muestras completas similares. Es costoso computacionalmente pero nos sirve tanto para variables categoricas como númericas al igual que el knn.
```{r}
#tempData <- mice(prepro,m=5,maxit=30,meth='pmm',seed=500)
#prepro_mice <- complete(tempData,1)
#(prepro_mice, file = "prepro_mice.Rdata")
load(file = 'prepro_mice.Rdata')
colSums(is.na(prepro_mice))
head(prepro_mice)
```

# Redes Neuronales

## Introducción teórica
https://www.xeridia.com/blog/redes-neuronales-artificiales-que-son-y-como-se-entrenan-parte-i
El concepto tras las redes neuronales no es complejo, pues se trata de un simple paralelismo respecto a su contexto biológico. Trata de imitar el funcionamiento de las redes neuronales de los organismos vivos: un conjunto de neuronas conectadas entre sí y que trabajan en conjunto, sin que haya una tarea concreta para cada una. Con la experiencia, las neuronas van creando y reforzando ciertas conexiones para "aprender" algo que se queda fijo en el tejido.

El ámbito teórico es mucho más complejo pero lo resumiremos hablando de su unidad mínima, el perceptrónel cuál recibe valores a los que aplica funciones de transformaciónque se irán ajustando según el algoritmo de retropropagación para así obetener salidas cada vez más cercanas a la esperada hasta resolver el problema propuesto. Varios perceptrones o neuronas se organizan en capas, cuantas más capas más compleja es nuestra red, no confundir con eficiente. De modo que para entrenar nuestra red la dotamos de ejemplos 'resueltos' train, que validaremos con los ejemplos de test para así ir ajustando los parámetros de las funciones de salida de cada una de las neuronas.

## Preprocesamiento adicional
La normalización de los datos de entrada es el proceso por el se normalizan todos los datos de entrada, es decir, se reducen a los rangos [0,1] o [-1,1]. Si no se realiza la normalización los datos de entrada tendrán un efecto adicional sobre la neurona, dando lugar a decisiones incorrectas

```{r}
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
prepro_knn_nor <- prepro_knn %>% mutate_if(is.numeric, normalize)
prepro_mice_nor <- prepro_mice %>% mutate_if(is.numeric, normalize)
prepro_withoutNA_nor <- prepro_withoutNA %>% mutate_if(is.numeric, normalize)
```

```{r}
prepro_knn_nor_d <- fastDummies::dummy_cols(prepro_knn_nor, select_columns = c("workclass","education","marital_status","occupation","relationship","race","sex","native_country","moreThan50K") )
```
En este caso probaremos como se comporta el modelo con los tres métodos anteoriormente expuestos para tratar los valores perdidos.
## Entrenamiento del modelo

## Validación del modelo

### Cálculo de valores de bondad

# Random Forest

## Introducción teórica

Un Random Forest es un ensemble que combina el uso de varios árboles de decisión como clasificador base, todo con el objetivo de obtener mejores resultados en nuestras predicciones.

La manera en la que funciona es la siguiente: supongamos que tenemos 500 árboles de decisión en nuestro Random Forest, cada uno de estos árboles va a devolver como salida una predicción para un ejemplo dado. Para computar la decisión final, se realiza una votación, de manera que la predicción con más votos es la que se tomará como la predicción del Random Forest.

La gracia de esto es que la manera en la que se forman los árboles de decisión que conforman nuestro Random Forest es distinta. Algunos de ellos tendrán en cuenta un subconjunto de atributos del dataset, y han sido entrenados con un subconjunto de las muestras totales. Esto no solo se realiza por temas de rendimiento en cuanto al tiempo de entrenamiento de estos árboles de decisión, sino que también para que no todos los árboles sean muy parecidos y tenga sentido plantearse realizar una votación por mayoría, evitando así el fenónemo de "overfitting". Esta manera de crear los árboles de decisión se conoce como "bootstraping".

## Entrenamiento del modelo

Lo primero de todo, vamos a establecer la semilla del generador de números aleatorios para que los resultados no varíen entre sucesivas ejecuciones. Además, vamos a realizar una división del dataset en entrenamiento y validación, otorgando un 70% del mismo a entrenamiento del modelo.

```{r}
set.seed(1234)
df <- prepro
split <- sample.split(df, SplitRatio = 0.7)
train <- subset(df, split == "TRUE")
test <- subset(df, split == "FALSE")
```

Una vez definida la división del dataset, vamos a crear el Random Forest, tomando como clase el atributo "moreThan50K". El parámetro importance=TRUE indica que tome solo en consideración los atributos que considere importantes.

```{r}
rf <- randomForest(moreThan50K ~ ., data = train, importance=TRUE)
```

Vamos a visualizar la importancia de las variables según el Random Forest que se nos ha generado:

```{r}
varImpPlot(rf, sort=TRUE)
```
Donde observamos que los atributos más importantes son el salario, la ocupación, el nivel educativo y la edad. Entre los atributos menos significativos, tenemos el género, la raza, el país de origen y el estado civil. En nuestro caso como no tenermos demasiados atributos los vamos a dejar, pero en casos donde el dataset tenga cientos de atributos es muy recomendable eliminar del Random Forest aquellos atributos que no sean considerados relevantes (sería una etapa en el análisis exploratorio de datos).

## Validación del modelo

Ahora, vamos a guardar en un dataframe las predicciones devueltas por nuestro Random Forest usando el conjunto de test. Como vamos a realizar un análisis ROC más adelante, indicamos que queremos que devuelva no solo la clase predicha, sino también la probabilidad de que ese ejemplo pertenezca a esa clase.

```{r}
rfPrediction = predict(rf, newdata=test, type="prob")
rfPrediction = as.data.frame(rfPrediction)
```

### Cálculo de valores de bondad

Con estas predicciones, vamos a usar el paquete uROC para dibujar la curva ROC.

```{r}
rocData <- roc(test$moreThan50K, rfPrediction$`<=50K`)
plot(rocData, cex = 2, main = "Random Forest: Curva ROC")
```
Como podemos observar, tiene muy buena pinta, porque la curva está localizada en la mitad superior de la recta y = x. De manera aproximada podemos ver que el área que deja bajo la curva es muy cercana a la unidad (la curva se comienza a parecer a un rectángulo aunque tiene el borde redondeado). Computemos el valor exacto del área bajo la curva (AUC)

```{r}
cat("Area under curve (AUC): ", auc(rocData))
```
Y vemos que es un valor muy cercano a la unidad, de forma que el Random Forest que hemos creado, aunque no hayamos tuneado los parámetros ni nada, ofrece unos resultados excelentes en las predicciones realizadas. Esta última tarea vamos a detallarla en el siguiente apartado para ver si podemos mejorar los resultados obtenidos.

## Tuneado de parámetros

TODO

# Support Vector Machine

## Introducción teórica

Las máquinas de vector soporte son clasificadores usados principalmente para tareas de clasificación binaria, aunque se pueden extrapolar a otro tipo de clasificaciones. Al entrenar un modelo de este tipo frente a unos datos de entrenamiento, lo que obtenemos es un discriminante, que no es más que el vector normal al hiperplano que separa las 2 clases, así como otros atributos necesarios para realizar la clasificación, que es prácticamente inmediata, ya que dado un nuevo ejemplo, basta con calcular el signo de este ejemplo con respecto al hiperplano calculado.

Por esa característica, la complejidad de este clasificador solo depende del número de filas del dataset, no del número de atributos que este contenga. En nuestro caso, con un dataset de dimensión 13, nos viene de perlas usar un clasificador así puesto que tendremos un incremento de velocidad con respecto a otros.

En el entrenamiento del modelo, para encontrar el hiperplano que separa a las 2 clases es necesario resolver un problema de optimización cuadrático, que tiene una complejidad espacial (el número de dimensiones del dataset) de O(N^2) y complejidad temporal (el número de filas del dataset) de O(N^3).

En ese problema de optimización, no sólo obtenemos el hiperplano, sino un margen "m" que trataremos que sea máximo para que el modelo sea lo más general posible. Este margen es la distancia del hiperplano a la primera instancia de los datos de entrenamiento.

En la terminología de este modelo, al hiperplano de separación se le conoce como máquina de vectores soporte y a las instancias que quedan dentro de los márgenes o en ellos se les llama vectores soporte.

![Imagen SVM](https://lh3.googleusercontent.com/proxy/Ckkbd5hQ3ZkE8Hs0I8oCggx_IoMOqpUkyXJH1FozSWSS6womo7N4bxdvIsTZ2bxLViiev4r5cRE3FhWNh81RsnNNOKtNxWCEUJHw0lEWSRuk_QmSayf3ZIU)

Como estamos tratando con hiperplanos, nuestro modelo es lineal, pero claro, nuestro problema puede no serlo. Para solucionar este problema usamos lo que se conoce como un "kernel", que no es más que una transformación no lineal que aplicamos a nuestro problema (nuestro dataset) para que sea linealmente separable, es decir, que podamos realizar la clasificación usando un hiperplano.

Existen varios kernels que podemos utilizar: polinomiales, radiales, lineales, sigmoides, gaussianos, cuadráticos inversos...

Al realizar esto nuestro nuevo problema linealmente separable tendra dimensiones mayores al problema original pero, como comentamos al principio, esto nos da igual porque la complejidad del modelo viene definida solamente por el número de filas del dataset, no del número de dimensiones (atributos).

![Imagen SVM Kernels](https://miro.medium.com/max/838/1*gXvhD4IomaC9Jb37tzDUVg.png)
En esta imagen, podemos observar cómo aplicando un kernel polinomial cuadrático transformamos un problema en 2 dimensiones a uno de 3 dimensiones, pero ahora linealmente separable.

## Tuneado básico de parámetros

Comencemos creando un split del dataset en 2 partes: entrenamiento y validación. Usaremos el 70% del dataset para entrenamiento y el 30% restante para validación. Además, estableceremos la semilla desde el principio para que la salida del análisis sea la misma siempre.

```{r}
set.seed(1234)
sample <- sample.split(prepro$moreThan50K, SplitRatio = 0.70) 
train = subset(prepro, sample == TRUE)
test = subset(prepro, sample == FALSE)
```

Ahora vamos a seleccionar entre un conjunto de valores de gamma cuál es el que nos da mejores resultados (un mejor valor de accuracy para nuestro modelo), para posteriormente usar ese valor en el entrenamiento usando K-fold, un método mucho más sofisticado que un split 70/30.

```{r}

gammaValues = c(0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1, 10, 100)

accuracies = list()
i = 1

for(gamma in gammaValues) {
  
  # Entrenamiento
  svmModel = svm(formula = moreThan50K ~ .,
                     data = train,
                     type = 'C-classification',
                     kernel = 'radial',
                     gamma = gamma)
  
  # Validación
  y_pred = predict(svmModel, newdata = test, type="response")
  
  # Cálculo de parámetros de bondad
  cm = table(test$moreThan50K, y_pred)
  accuracy = (cm[1,1] + cm[2,2]) / (cm[1,1] + cm[2,2] + cm[1,2] + cm[2,1])
  
  accuracies[[i]] = accuracy;
  i = i + 1
}

accuracies <- unlist(accuracies)

```

Ahora que tenemos las medidas de "accuracy" correspondiente a cada valor de gamma, veamos cuál de esos valores nos da un mejor resultado

```{r}

ggplot(data.frame(gammaValues, accuracies), aes(x=gammaValues, y=accuracies)) + geom_point() + geom_line()

```

```{r}

bestAccIndex = which.max(accuracies)
bestGamma = gammaValues[bestAccIndex]
cat("Best gamma: ", bestGamma, "\n")
cat("Best accuracy: ", accuracies[[bestAccIndex]], "\n")

```
Observamos que el valor de gamma = 0.2 nos da el mejor resultado, con un 84% de accuracy. Entonces, usaremos ese valor para realizar el entrenamiento del modelo.

## Entrenamiento y validación del modelo

Como se comentó antes, vamos a usar K-Fold Cross Validation para realizar el entrenamiento, de manera que los resultados de valores de bondad sean lo más independientes posibles del split de entrenamiento y validación que usemos. En este caso, hemos usado k = 8 como un valor razonable en tiempos de cómputo y fiabilidad de los resultados. Además, para demostrar conocimiento en el tema, se ha programado desde 0 el método K-Fold CV. La máquina de vector soporte usada tiene un kernel radial.

```{r}
kfold_k = 8
folds = createFolds(train$moreThan50K, k = kfold_k)
  
cv = lapply(folds, function(x) {
  
  # Separación del conjunto train en k partes
  training_fold = train[-x, ]
  test_fold = train[x, ]
    
  # Entrenamiento en ese fold
  svmModel = svm(formula = moreThan50K ~ .,
                 data = training_fold,
                 type = 'C-classification',
                 kernel = 'radial',
                 gamma = bestGamma)
  
  # Cálculo de parámetros de bondad
  y_pred = predict(svmModel, newdata = test_fold, type="response")
  cm = table(test_fold$moreThan50K, y_pred)
  accuracy = (cm[1,1] + cm[2,2]) / (cm[1,1] + cm[2,2] + cm[1,2] + cm[2,1])
  precision = recall = cm[1,1] / (cm[1,1] + cm[1,2])
  recall = cm[1,1] / (cm[1,1] + cm[2,1])
  specificity = cm[2,2] / (cm[2,2] + cm[1,2])
  return(c(accuracy, precision, recall, specificity))
})
  
# Cálculo de parámetros
params = c(0, 0, 0, 0)

for (fold in cv) {
  params = params + fold
}
  
params = params / kfold_k

```

Una vez entrenado el modelo, validado y calculados los parámetros, podemos visualizarlos. En este caso, no tiene sentido mostrar la matriz de confusión puesto que en cada fold obtenemos una matriz de confusión diferente. Los parámetros de bondad son simplemente las medias de los parámetros de bondad de cada fold. Además, como la clasificación solo devuelve la predicción de la clase y no la probabilidad, no podemos realizar un análisis ROC en este caso.

```{r}

cat("Accuracy: ", params[1], "\n")
cat("Precision: ", params[2], "\n")
cat("Recall: ", params[3], "\n")
cat("Specificity: ", params[4], "\n")

```
Vemos que el accuracy es del 83% y una precisión del 87%, resultados que son bastante buenos teniendo en cuenta la velocidad de predicción que tiene este clasificador.

# Comparativa entre los modelos

# Conclusiones

# Bibliografía

[1] Dataset: https://www.kaggle.com/uciml/adult-census-income
[2] Parámetro fnlwgt: https://www.kaggle.com/uciml/adult-census-income/discussion/32698
[3] Imagen máquina de vector soporte: http://numerentur.org/svm/
[4] Imagen kernels de máquina vector soporte: https://medium.com/analytics-vidhya/how-to-classify-non-linear-data-to-linear-data-bb2df1a6b781
